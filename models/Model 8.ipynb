{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3554f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from typing import List\n",
    "\n",
    "NUM_WORDS = 5000\n",
    "NUM_AUTHORS = 21246\n",
    "MAX_LEN = 250\n",
    "RANDOM_STATE = 42069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6776af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(path: str):\n",
    "    \"\"\"\n",
    "    loads data set located at path and returns as pandas data frame\n",
    "    \"\"\"\n",
    "    with open(path) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    print(f\"loaded {len(data)} instances\")\n",
    "    data = pd.json_normalize(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb1299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 25793 instances\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>venue</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[42, 13720, 36]</td>\n",
       "      <td>9</td>\n",
       "      <td>[2455, 1858, 2335, 1543, 1800, 1860, 2000, 286...</td>\n",
       "      <td>20</td>\n",
       "      <td>[41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1359, 15881, 45]</td>\n",
       "      <td>15</td>\n",
       "      <td>[40, 1542, 1691, 2449, 1535, 3616, 2206, 1904,...</td>\n",
       "      <td>2</td>\n",
       "      <td>[1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[19166, 17763]</td>\n",
       "      <td>17</td>\n",
       "      <td>[40, 1542, 1691, 2449, 1535, 2610, 1543, 1535,...</td>\n",
       "      <td></td>\n",
       "      <td>[2085, 1719, 1846, 1745, 2243, 1553, 1606, 159...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[97]</td>\n",
       "      <td>10</td>\n",
       "      <td>[46, 1624, 1547, 56, 1687, 1644, 6, 7, 3386, 1...</td>\n",
       "      <td>4</td>\n",
       "      <td>[40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[19617, 2]</td>\n",
       "      <td>10</td>\n",
       "      <td>[37, 3709, 3836, 1586, 2151, 1727, 3021, 1860,...</td>\n",
       "      <td>9</td>\n",
       "      <td>[38, 1592, 2088, 1543, 1574, 1727, 1597, 1813,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             authors  year                                           abstract  \\\n",
       "0    [42, 13720, 36]     9  [2455, 1858, 2335, 1543, 1800, 1860, 2000, 286...   \n",
       "1  [1359, 15881, 45]    15  [40, 1542, 1691, 2449, 1535, 3616, 2206, 1904,...   \n",
       "2     [19166, 17763]    17  [40, 1542, 1691, 2449, 1535, 2610, 1543, 1535,...   \n",
       "3               [97]    10  [46, 1624, 1547, 56, 1687, 1644, 6, 7, 3386, 1...   \n",
       "4         [19617, 2]    10  [37, 3709, 3836, 1586, 2151, 1727, 3021, 1860,...   \n",
       "\n",
       "  venue                                              title  \n",
       "0    20  [41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...  \n",
       "1     2  [1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...  \n",
       "2        [2085, 1719, 1846, 1745, 2243, 1553, 1606, 159...  \n",
       "3     4  [40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...  \n",
       "4     9  [38, 1592, 2088, 1543, 1574, 1727, 1597, 1813,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/train.json\"\n",
    "train = load_data_set(path)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb4104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, train=True, drop_samples=False):\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "   \n",
    "    if train:\n",
    "        df[\"target authors\"] = df[\"authors\"].apply(lambda x: filter_authors(x))\n",
    "        df[\"coauthors\"]      = df[\"authors\"].apply(lambda x: filter_authors(x, prolifics=False))\n",
    "        df = df.drop([\"authors\"], axis=1)\n",
    "    \n",
    "    # drops samples containing no prolific authors, Reduces training set by ~60% to 7000 samples\n",
    "    if drop_samples:\n",
    "        df[\"has target\"] = df[\"target authors\"].apply(lambda x: len(x)>0)\n",
    "        df = df[df[\"has target\"] == True]\n",
    "        df = df.drop([\"has target\"], axis=1)\n",
    "        \n",
    "    # text transormation\n",
    "    # we stringify the list of int's to be used as inputs to the TF-IDF vectoriser\n",
    "    df[\"text\"] = df[\"title\"] + df[\"abstract\"]\n",
    "    df[\"str text\"] = df[\"text\"].apply(lambda xs: ''.join(str(x)+' ' for x in xs))\n",
    "    \n",
    "    # preprocessing for venue. We use minmax scaling as a matter of best-practice. \n",
    "    # as we require all rows to have integer values, we give blank venues a dummy value of 465\n",
    "    scalar = MinMaxScaler()\n",
    "    df.loc[df.venue == \"\", \"venue\"] = 465\n",
    "    df[\"venue\"] = scalar.fit_transform(df[\"venue\"].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    # prepocessing for coauthors\n",
    "    # we use a discretised binning strategy, with n=10 bins by default. \n",
    "    df[\"coauthors\"] = df[\"coauthors\"].apply(lambda x: build_bins(x, n_bins=10))\n",
    "    coauth_df = pd.DataFrame(df.coauthors.tolist(), index=df.index, columns=[\"bin \"+str(i) for i in range(10)])\n",
    "\n",
    "    # drop\n",
    "    df = df.drop([\"abstract\", \"title\", \"year\"], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa61f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature transformation\n",
    "\n",
    "def filter_authors(authors: List[int], prolifics=True):\n",
    "    \"\"\"\n",
    "    filters authors between prolific and coauthors\n",
    "    \"\"\"\n",
    "    if prolifics:\n",
    "        prolifics = filter(lambda x: x < 100, authors)\n",
    "        return list(prolifics)\n",
    "    else:\n",
    "        coauthors = filter(lambda x: x>=100, authors)\n",
    "        return list(coauthors)\n",
    "    \n",
    "def build_bins(coauthors: List[int], n_bins=10):\n",
    "    \"\"\"\n",
    "    takes a list of coauthors and returns 10-column data frame\n",
    "    \n",
    "    This might be some of the uggliest code I have ever written, though\n",
    "    sklearn's discrete bins didn't really give what I wanted\n",
    "    \"\"\"\n",
    "    width = np.ceil(21246/n_bins)\n",
    "    bins  = np.zeros(n_bins)\n",
    "    for author in coauthors:\n",
    "        i = 0\n",
    "        while not (max(0,(i-1))*width <= author <= i*width):\n",
    "            i += 1\n",
    "        bins[i-1] += 1\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd5eb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue</th>\n",
       "      <th>target authors</th>\n",
       "      <th>coauthors</th>\n",
       "      <th>text</th>\n",
       "      <th>str text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043011</td>\n",
       "      <td>[42, 36]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...</td>\n",
       "      <td>[41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...</td>\n",
       "      <td>41 1550 1563 1594 1544 1919 1644 37 1539 1715 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004301</td>\n",
       "      <td>[45]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "      <td>[1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...</td>\n",
       "      <td>1731 47 11 57 4624 1525 1535 47 11 3522 2223 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008602</td>\n",
       "      <td>[97]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...</td>\n",
       "      <td>40 1733 1735 1540 1655 46 1624 1547 56 1687 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019355</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[38, 1592, 2088, 1543, 1574, 1727, 1597, 1813,...</td>\n",
       "      <td>38 1592 2088 1543 1574 1727 1597 1813 1926 152...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>[44, 2]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[1560, 1694, 11, 1546, 11, 3066, 1728, 47, 160...</td>\n",
       "      <td>1560 1694 11 1546 11 3066 1728 47 1603 1553 11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      venue target authors                                          coauthors  \\\n",
       "0  0.043011       [42, 36]  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...   \n",
       "1  0.004301           [45]  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...   \n",
       "3  0.008602           [97]  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  0.019355            [2]  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  0.000000        [44, 2]  [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  [41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...   \n",
       "1  [1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...   \n",
       "3  [40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...   \n",
       "4  [38, 1592, 2088, 1543, 1574, 1727, 1597, 1813,...   \n",
       "9  [1560, 1694, 11, 1546, 11, 3066, 1728, 47, 160...   \n",
       "\n",
       "                                            str text  \n",
       "0  41 1550 1563 1594 1544 1919 1644 37 1539 1715 ...  \n",
       "1  1731 47 11 57 4624 1525 1535 47 11 3522 2223 1...  \n",
       "3  40 1733 1735 1540 1655 46 1624 1547 56 1687 16...  \n",
       "4  38 1592 2088 1543 1574 1727 1597 1813 1926 152...  \n",
       "9  1560 1694 11 1546 11 3066 1728 47 1603 1553 11...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess(train, drop_samples=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23e5e5",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "TF-IDF is a nlp preprocessing method to map text input to a vector of reals. TF-IDF is an improvement upon previous feature engineering that we have performed as it adjusts the value of each word, relative to how freuently it occurs. Stop words such as *the, it, how* have relatively low weightings, so the resulting vector only captures the most *important* words within the input. This (typically) leads to TF-IDF representations outperforming word-count representations for most tasks\n",
    "\n",
    "We use `sklearn`'s feature extraction to automate this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf   = TfidfVectorizer()\n",
    "vectors = tfidf.fit_transform(df[\"str text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df_text = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d2f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2614ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_auth = df[\"target authors\"]\n",
    "df_full = pd.concat([df_text.reset_index(drop=True), df_auth.reset_index(drop=True)], axis=1)\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca22b4",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "let's assess results for a single author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc17e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_training(X_train, y_train):\n",
    "    \"\"\"\n",
    "    upsamples the minority class until class balance is achieved\n",
    "    \"\"\"\n",
    "    X = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    \n",
    "    pos = X[X[\"label\"] == 1]\n",
    "    neg = X[X[\"label\"] == 0]\n",
    "    \n",
    "    pos_upsample = resample(pos, replace=True, n_samples=len(neg), random_state=RANDOM_STATE)\n",
    "    \n",
    "    resampled = pd.concat([neg, pos_upsample])\n",
    "\n",
    "    y_train = resampled[\"label\"]\n",
    "    X_train = resampled.drop([\"label\"], axis=1)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def resample_training(X_train, y_train):\n",
    "    \"\"\"\n",
    "    resamples class imbalance using SMOTE: \n",
    "    https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "    \"\"\"\n",
    "    sm = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_data(author: int, df:pd.DataFrame):\n",
    "    # take copy and prepare label\n",
    "    df = df.copy(deep=True)\n",
    "    df[\"label\"] = df[\"target authors\"].apply(lambda x: 1 if author in x else 0)\n",
    "    X = df.drop([\"label\", \"target authors\"], axis=1)\n",
    "    y = df[\"label\"]\n",
    "    # split training and validation - we have fixed random state for reproducability\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # upsample to deal with class imbalance\n",
    "    X_train, y_train = upsample_training(X_train, y_train)\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05574a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_author = 25\n",
    "data        = df_full\n",
    "X_train, X_val, y_train, y_val = get_train_val_data(test_author, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a5dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb91124",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa7d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a858fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy: {accuracy_score(y_preds, y_val)}\")\n",
    "print(f\"f1      : {f1_score(y_preds, y_val)}\")\n",
    "print(\"confusion matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_preds, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ed202",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = RandomForestClassifier(n_estimators=1000, max_depth=6)\n",
    "clf3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = clf3.predict(X_val)\n",
    "print(f\"accuracy: {accuracy_score(y_preds, y_val)}\")\n",
    "print(f\"f1      : {f1_score(y_preds, y_val)}\")\n",
    "print(\"confusion matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_preds, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c94558",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = GradientBoostingClassifier()\n",
    "clf4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = clf4.predict(X_val)\n",
    "print(f\"accuracy: {accuracy_score(y_preds, y_val)}\")\n",
    "print(f\"f1      : {f1_score(y_preds, y_val)}\")\n",
    "print(\"confusion matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_preds, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6fab49",
   "metadata": {},
   "source": [
    "## Building the binary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231064ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(author: int, df: pd.DataFrame):\n",
    "    df = df.copy(deep=True)\n",
    "    df[\"label\"] = df[\"target authors\"].apply(lambda x: 1 if author in x else 0)\n",
    "    X = df.drop([\"label\", \"target authors\"], axis=1)\n",
    "    y = df[\"label\"]\n",
    "    \n",
    "    X, y = resample_training(X, y)\n",
    "    \n",
    "    clf = LogisticRegression(penalty='l1',solver='liblinear')\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "authors = np.arange(100)\n",
    "\n",
    "for author in tqdm(authors):\n",
    "    classifier = build_classifier(author, df_full)\n",
    "    models.append(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e921e",
   "metadata": {},
   "source": [
    "## Building the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/test.json\"\n",
    "df_test = load_data_set(path)\n",
    "df_test[\"text\"] = df_test[\"title\"] + df_test[\"abstract\"]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a72cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now need to filter out the text column based on our TF-IDF features\n",
    "tfidf_features = tfidf.get_feature_names_out()\n",
    "tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this complicated looking lambda simply removes any words that were not part of our preprocessing. \n",
    "# failing to do so, would pass an unseen word to our tf-idf vectoriser and would crash our program\n",
    "df_test['text'] = df_test['text'].apply(lambda xs: list(filter((lambda x: str(x) in tfidf_features), xs)))\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda xs: ''.join(str(x)+' ' for x in xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6501c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf.transform(df_test['text'])\n",
    "X_test = pd.DataFrame((X_test.todense().tolist()), columns=tfidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    function for writing predictions to output file. \n",
    "    WARNING: Deletes predictions.csv if present in working directory\n",
    "    \"\"\"\n",
    "    if os.path.exists(\"predictions.csv\"):\n",
    "        os.remove(\"predictions.csv\")\n",
    "        print(\"removed previous predictions\")\n",
    "    \n",
    "    \n",
    "    with open(\"predictions.csv\", mode='w') as f:    \n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        header = ['Id','Predict']\n",
    "        writer.writerow(header)\n",
    "        n      = X_test.shape[0]\n",
    "        \n",
    "        # loop over each training sample and write to necessary format\n",
    "        for Id in tqdm(range(n)):\n",
    "            x   = np.array(X_test.iloc[Id]).reshape(1, -1)\n",
    "            row = [Id]\n",
    "            authors = \"\"\n",
    "            for author, model in enumerate(models):\n",
    "                if np.array(model.predict(x)).item() == 1:\n",
    "                    authors += str(author) + \" \"\n",
    "\n",
    "            # to match the output requirement \n",
    "            if len(authors) == 0: row = [Id, -1]\n",
    "            else: row = [Id, authors]\n",
    "            \n",
    "            writer.writerow(row)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0373c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
