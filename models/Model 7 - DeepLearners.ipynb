{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b0f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from typing import List\n",
    "\n",
    "NUM_WORDS = 5001\n",
    "PADDING   = 5000    # special char set as padding\n",
    "NUM_AUTHORS = 21246\n",
    "MAX_LEN = 250\n",
    "RANDOM_STATE = 42069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7303b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(path: str):\n",
    "    \"\"\"\n",
    "    loads data set located at path and returns as pandas data frame\n",
    "    \"\"\"\n",
    "    with open(path) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    print(f\"loaded {len(data)} instances\")\n",
    "    data = pd.json_normalize(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d56a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 25793 instances\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>venue</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[42, 13720, 36]</td>\n",
       "      <td>9</td>\n",
       "      <td>[2455, 1858, 2335, 1543, 1800, 1860, 2000, 286...</td>\n",
       "      <td>20</td>\n",
       "      <td>[41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1359, 15881, 45]</td>\n",
       "      <td>15</td>\n",
       "      <td>[40, 1542, 1691, 2449, 1535, 3616, 2206, 1904,...</td>\n",
       "      <td>2</td>\n",
       "      <td>[1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[19166, 17763]</td>\n",
       "      <td>17</td>\n",
       "      <td>[40, 1542, 1691, 2449, 1535, 2610, 1543, 1535,...</td>\n",
       "      <td></td>\n",
       "      <td>[2085, 1719, 1846, 1745, 2243, 1553, 1606, 159...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[97]</td>\n",
       "      <td>10</td>\n",
       "      <td>[46, 1624, 1547, 56, 1687, 1644, 6, 7, 3386, 1...</td>\n",
       "      <td>4</td>\n",
       "      <td>[40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[19617, 2]</td>\n",
       "      <td>10</td>\n",
       "      <td>[37, 3709, 3836, 1586, 2151, 1727, 3021, 1860,...</td>\n",
       "      <td>9</td>\n",
       "      <td>[38, 1592, 2088, 1543, 1574, 1727, 1597, 1813,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             authors  year                                           abstract  \\\n",
       "0    [42, 13720, 36]     9  [2455, 1858, 2335, 1543, 1800, 1860, 2000, 286...   \n",
       "1  [1359, 15881, 45]    15  [40, 1542, 1691, 2449, 1535, 3616, 2206, 1904,...   \n",
       "2     [19166, 17763]    17  [40, 1542, 1691, 2449, 1535, 2610, 1543, 1535,...   \n",
       "3               [97]    10  [46, 1624, 1547, 56, 1687, 1644, 6, 7, 3386, 1...   \n",
       "4         [19617, 2]    10  [37, 3709, 3836, 1586, 2151, 1727, 3021, 1860,...   \n",
       "\n",
       "  venue                                              title  \n",
       "0    20  [41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...  \n",
       "1     2  [1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...  \n",
       "2        [2085, 1719, 1846, 1745, 2243, 1553, 1606, 159...  \n",
       "3     4  [40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...  \n",
       "4     9  [38, 1592, 2088, 1543, 1574, 1727, 1597, 1813,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/train.json\"\n",
    "train = load_data_set(path)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2708e0",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1833da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, train=True, drop_samples=False):\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "   \n",
    "    if train:\n",
    "        df[\"target authors\"] = df[\"authors\"].apply(lambda x: filter_authors(x))\n",
    "        df[\"coauthors\"]      = df[\"authors\"].apply(lambda x: filter_authors(x, prolifics=False))\n",
    "        df = df.drop([\"authors\"], axis=1)\n",
    "    \n",
    "    # drops samples containing no prolific authors, Reduces training set by ~60% to 7000 samples\n",
    "    if drop_samples:\n",
    "        df[\"has target\"] = df[\"target authors\"].apply(lambda x: len(x)>0)\n",
    "        df = df[df[\"has target\"] == True]\n",
    "        df = df.drop([\"has target\"], axis=1)\n",
    "        \n",
    "    # text transormation\n",
    "    df[\"text\"] = df[\"title\"] + df[\"abstract\"]\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: pad_sequence(x, MAX_LEN))\n",
    "    \n",
    "    # drop unnecessarily long examples\n",
    "    if train:\n",
    "        df[\"len\"] = df[\"text\"].apply(lambda x: len(x))\n",
    "        df = df[df[\"len\"]<=MAX_LEN]\n",
    "\n",
    "    # drop\n",
    "    df = df.drop([\"abstract\", \"title\", \"year\", \"coauthors\", \"venue\", \"len\"], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5020f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature transformation\n",
    "\n",
    "def filter_authors(authors: List[int], prolifics=True):\n",
    "    \"\"\"\n",
    "    filters authors between prolific and coauthors\n",
    "    \"\"\"\n",
    "    if prolifics:\n",
    "        prolifics = filter(lambda x: x < 100, authors)\n",
    "        return list(prolifics)\n",
    "    else:\n",
    "        coauthors = filter(lambda x: x>=100, authors)\n",
    "        return list(coauthors)\n",
    "    \n",
    "    \n",
    "def pad_sequence(sequence: List, max_len: int):\n",
    "    return sequence + [PADDING] * (max_len - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8805ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target authors</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[42, 36]</td>\n",
       "      <td>[41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[45]</td>\n",
       "      <td>[1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[97]</td>\n",
       "      <td>[40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2]</td>\n",
       "      <td>[38, 1592, 2088, 1543, 1574, 1727, 1597, 1813,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[44, 2]</td>\n",
       "      <td>[1560, 1694, 11, 1546, 11, 3066, 1728, 47, 160...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target authors                                               text\n",
       "0       [42, 36]  [41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...\n",
       "1           [45]  [1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...\n",
       "3           [97]  [40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...\n",
       "4            [2]  [38, 1592, 2088, 1543, 1574, 1727, 1597, 1813,...\n",
       "9        [44, 2]  [1560, 1694, 11, 1546, 11, 3066, 1728, 47, 160..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess(train, drop_samples=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55eb0e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6690, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2bd1e6",
   "metadata": {},
   "source": [
    "## Training - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2457f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_eval_split(author: int, df: pd.DataFrame, resampling=None):\n",
    "    df = df.copy(deep=True)\n",
    "    df[\"label\"] = df[\"target authors\"].apply(lambda x: 1 if author in x else 0)\n",
    "    \n",
    "    X, y = df[\"text\"], df[\"label\"]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
    "    \n",
    "    if resampling:\n",
    "        X_train, y_train = resampling(X_train, y_train)\n",
    "        X_train, y_train = X_train.to_numpy(), y_train.to_numpy()\n",
    "    \n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecdccd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training - test split for a single author for prototyping\n",
    "test_auth = 69\n",
    "X_train, X_val, y_train, y_val = training_eval_split(test_auth, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a47faf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1739     [37, 1554, 2461, 1556, 4129, 1544, 2625, 1787,...\n",
       "1101     [11, 3717, 1623, 1575, 1542, 2125, 1547, 2174,...\n",
       "9351     [46, 1670, 1547, 1549, 4905, 1569, 1820, 1543,...\n",
       "681      [4284, 3231, 2445, 1553, 2258, 11, 1669, 2771,...\n",
       "24205    [1560, 40, 2148, 11, 1994, 24, 3228, 1549, 425...\n",
       "                               ...                        \n",
       "2130     [47, 1603, 1538, 3591, 1533, 1780, 1708, 1539,...\n",
       "6782     [2600, 3057, 1546, 2590, 1608, 11, 3580, 3929,...\n",
       "369      [1779, 1660, 1747, 34, 24, 47, 1751, 4326, 155...\n",
       "17249    [1542, 2667, 1854, 3591, 1747, 1671, 1575, 160...\n",
       "22618    [40, 2211, 37, 1596, 1567, 1541, 1708, 1704, 1...\n",
       "Name: text, Length: 2007, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ae4ee06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3128     [46, 1574, 1660, 1661, 2796, 1547, 1543, 1830,...\n",
       "11800    [3390, 11, 1564, 1661, 2193, 41, 3949, 1745, 5...\n",
       "14967    [11, 37, 1740, 1623, 1621, 50, 1620, 1632, 57,...\n",
       "14553    [48, 1559, 2379, 1826, 33, 37, 2085, 11, 57, 4...\n",
       "5338     [11, 37, 3516, 24, 57, 46, 1899, 1567, 46, 167...\n",
       "                               ...                        \n",
       "1342     [38, 2105, 1551, 37, 1766, 1553, 46, 2941, 50,...\n",
       "9814     [31, 11, 24, 37, 39, 3015, 1553, 2082, 1545, 1...\n",
       "18813    [1539, 1551, 3485, 1573, 1715, 1553, 47, 2324,...\n",
       "14540    [47, 1806, 46, 1991, 1553, 54, 1551, 41, 1570,...\n",
       "5369     [1784, 1560, 2446, 1527, 11, 1560, 2197, 1538,...\n",
       "Name: text, Length: 4683, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971ad21",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "This model is based off of the post at: https://medium.com/analytics-vidhya/multiclass-text-classification-using-deep-learning-f25b4b1010e5\n",
    "within, the author uses `GloVE` word-embedding which we replicate here. \n",
    "\n",
    "This matrix is used as an embedding matrix for our neural networks to act as a sort of feature normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc228ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3437: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "100%|██████████| 5001/5001 [00:00<00:00, 120404.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../GloVE/glove.840B.300d.txt'\n",
    "\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "    nb_words = min(MAX_FEATURES, len(word_index)+1)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= MAX_FEATURES: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "word_index = {}\n",
    "for i in range(5001):\n",
    "    word_index[i] = i\n",
    "\n",
    "MAX_FEATURES = NUM_WORDS\n",
    "embedding_matrix = load_glove(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d02d47c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5001, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d24b0a",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c28627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Text(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        filter_sizes = [1,2,3,5]\n",
    "        num_filters = 36\n",
    "        n_classes = len(le.classes_)\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)\n",
    "def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  \n",
    "        logit = self.fc1(x)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b84e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = 64\n",
    "        drp = 0.1\n",
    "        n_classes = 2\n",
    "        self.embedding = nn.Embedding(NUM_WORDS, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size*4 , 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drp)\n",
    "        self.out = nn.Linear(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        avg_pool = torch.mean(h_lstm, 1)\n",
    "        max_pool, _ = torch.max(h_lstm, 1)\n",
    "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69981d2b",
   "metadata": {},
   "source": [
    "## Training & Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72326314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-1f4f6feab860>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.long)\n",
      "94it [00:22,  4.10it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6 \t loss=4.9593 \t val_loss=3.7576  \t val_acc=0.9856 \t f1 = 0.0000  \t time=28.40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [00:22,  4.09it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/6 \t loss=2.9254 \t val_loss=3.7078  \t val_acc=0.9856 \t f1 = 0.0000  \t time=28.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [00:22,  4.09it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/6 \t loss=2.8459 \t val_loss=3.7578  \t val_acc=0.9856 \t f1 = 0.0000  \t time=28.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [00:23,  4.07it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6 \t loss=2.7472 \t val_loss=3.4324  \t val_acc=0.9856 \t f1 = 0.0000  \t time=28.48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [00:23,  4.06it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/6 \t loss=2.3743 \t val_loss=3.6464  \t val_acc=0.9856 \t f1 = 0.0000  \t time=28.56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [00:23,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/6 \t loss=1.8547 \t val_loss=2.7985  \t val_acc=0.9856 \t f1 = 0.0000  \t time=28.57s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n_epochs = 6\n",
    "batch_size = 50\n",
    "\n",
    "embed_size = 300\n",
    "model = BiLSTM()\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "# convert series to lists\n",
    "X_val_list = X_val.to_list()\n",
    "y_val_list = y_val.to_list()\n",
    "\n",
    "# Load train and test in CUDA Memory\n",
    "x_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "x_cv = torch.tensor(X_val_list, dtype=torch.long)\n",
    "y_cv = torch.tensor(y_val_list, dtype=torch.long)\n",
    "\n",
    "# Create Torch datasets\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "valid = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    # Set model to train configuration\n",
    "    model.train()\n",
    "    avg_loss = 0.  \n",
    "    for i, (x_batch, y_batch) in tqdm(enumerate(train_loader)):\n",
    "        # Predict/Forward Pass\n",
    "        y_pred = model(x_batch)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    \n",
    "    # Set model to validation configuration - Doesn't get trained here\n",
    "    model.eval()        \n",
    "    avg_val_loss = 0.\n",
    "    val_preds = np.zeros((len(x_cv), 2))\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        # keep/store predictions\n",
    "        val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Check Accuracy\n",
    "    preds = val_preds.argmax(axis=1)\n",
    "    val_accuracy = sum(preds==y_val)/len(y_val)\n",
    "    f1 = f1_score(y_val, preds)\n",
    "    train_loss.append(avg_loss)\n",
    "    valid_loss.append(avg_val_loss)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f} \\t f1 = {:.4f}  \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, f1, elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb952860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97f75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
