{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_WORDS = 5000\n",
    "NUM_AUTHORS = 21246\n",
    "MAX_LEN = 250\n",
    "RANDOM_STATE = 42069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set(path: str):\n",
    "    \"\"\"\n",
    "    loads data set located at path and returns as pandas data frame\n",
    "    \"\"\"\n",
    "    with open(path) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    print(f\"loaded {len(data)} instances\")\n",
    "    data = pd.json_normalize(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 25793 instances\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>authors</th>\n      <th>year</th>\n      <th>abstract</th>\n      <th>venue</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[42, 13720, 36]</td>\n      <td>9</td>\n      <td>[2455, 1858, 2335, 1543, 1800, 1860, 2000, 286...</td>\n      <td>20</td>\n      <td>[41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[1359, 15881, 45]</td>\n      <td>15</td>\n      <td>[40, 1542, 1691, 2449, 1535, 3616, 2206, 1904,...</td>\n      <td>2</td>\n      <td>[1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[19166, 17763]</td>\n      <td>17</td>\n      <td>[40, 1542, 1691, 2449, 1535, 2610, 1543, 1535,...</td>\n      <td></td>\n      <td>[2085, 1719, 1846, 1745, 2243, 1553, 1606, 159...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[97]</td>\n      <td>10</td>\n      <td>[46, 1624, 1547, 56, 1687, 1644, 6, 7, 3386, 1...</td>\n      <td>4</td>\n      <td>[40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[19617, 2]</td>\n      <td>10</td>\n      <td>[37, 3709, 3836, 1586, 2151, 1727, 3021, 1860,...</td>\n      <td>9</td>\n      <td>[38, 1592, 2088, 1543, 1574, 1727, 1597, 1813,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "             authors  year                                           abstract  \\\n0    [42, 13720, 36]     9  [2455, 1858, 2335, 1543, 1800, 1860, 2000, 286...   \n1  [1359, 15881, 45]    15  [40, 1542, 1691, 2449, 1535, 3616, 2206, 1904,...   \n2     [19166, 17763]    17  [40, 1542, 1691, 2449, 1535, 2610, 1543, 1535,...   \n3               [97]    10  [46, 1624, 1547, 56, 1687, 1644, 6, 7, 3386, 1...   \n4         [19617, 2]    10  [37, 3709, 3836, 1586, 2151, 1727, 3021, 1860,...   \n\n  venue                                              title  \n0    20  [41, 1550, 1563, 1594, 1544, 1919, 1644, 37, 1...  \n1     2  [1731, 47, 11, 57, 4624, 1525, 1535, 47, 11, 3...  \n2        [2085, 1719, 1846, 1745, 2243, 1553, 1606, 159...  \n3     4  [40, 1733, 1735, 1540, 1655, 46, 1624, 1547, 5...  \n4     9  [38, 1592, 2088, 1543, 1574, 1727, 1597, 1813,...  "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"train.json\"\n",
    "train = load_data_set(path)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, train=True, drop_samples=False):\n",
    "    \n",
    "    df = df.copy(deep=True)\n",
    "   \n",
    "    if train:\n",
    "        df[\"target authors\"] = df[\"authors\"].apply(lambda x: filter_authors(x))\n",
    "        df[\"coauthors\"]      = df[\"authors\"].apply(lambda x: filter_authors(x, prolifics=False))\n",
    "        df = df.drop([\"authors\"], axis=1)\n",
    "    \n",
    "    # drops samples containing no prolific authors, Reduces training set by ~60% to 7000 samples\n",
    "    if drop_samples:\n",
    "        df[\"has target\"] = df[\"target authors\"].apply(lambda x: len(x)>0)\n",
    "        df = df[df[\"has target\"] == True]\n",
    "        df = df.drop([\"has target\"], axis=1)\n",
    "        \n",
    "    # text transormation\n",
    "    # we stringify the list of int's to be used as inputs to the TF-IDF vectoriser\n",
    "    df[\"text\"] = df[\"title\"] + df[\"abstract\"]\n",
    "    df[\"str text\"] = df[\"text\"].apply(lambda xs: ''.join(str(x)+' ' for x in xs))\n",
    "    \n",
    "    # preprocessing for venue. We use minmax scaling as a matter of best-practice. \n",
    "    # as we require all rows to have integer values, we give blank venues a dummy value of 465\n",
    "    scalar = MinMaxScaler()\n",
    "    df.loc[df.venue == \"\", \"venue\"] = 465\n",
    "    df[\"venue\"] = scalar.fit_transform(df[\"venue\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    # drop\n",
    "    df = df.drop([\"abstract\", \"title\"], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_authors(authors: List[int], prolifics=True):\n",
    "    \"\"\"\n",
    "    filters authors between prolific and coauthors\n",
    "    \"\"\"\n",
    "    if prolifics:\n",
    "        prolifics = filter(lambda x: x < 100, authors)\n",
    "        return list(prolifics)\n",
    "    else:\n",
    "        coauthors = filter(lambda x: x>=100, authors)\n",
    "        return list(coauthors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(train, train=True, drop_samples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 9, 15, 10, 18, 11,  8,  2, 13,  1, 16,  6,  5, 17, 14,  3, 12,  4,\n        7,  0])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"year\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Preprocessing\n",
    "\n",
    "TF-IDF is a nlp preprocessing method to map text input to a vector of reals. TF-IDF is an improvement upon previous feature engineering that we have performed as it adjusts the value of each word, relative to how freuently it occurs. Stop words such as *the, it, how* have relatively low weightings, so the resulting vector only captures the most *important* words within the input. This (typically) leads to TF-IDF representations outperforming word-count representations for most tasks\n",
    "\n",
    "We use `sklearn`'s feature extraction to automate this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf   = TfidfVectorizer()\n",
    "vectors = tfidf.fit_transform(df[\"str text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>10</th>\n      <th>100</th>\n      <th>1005</th>\n      <th>1006</th>\n      <th>1007</th>\n      <th>1009</th>\n      <th>101</th>\n      <th>1014</th>\n      <th>1016</th>\n      <th>1022</th>\n      <th>...</th>\n      <th>962</th>\n      <th>965</th>\n      <th>968</th>\n      <th>970</th>\n      <th>973</th>\n      <th>977</th>\n      <th>98</th>\n      <th>980</th>\n      <th>987</th>\n      <th>998</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.090374</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.018773</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.130861</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 3869 columns</p>\n</div>",
      "text/plain": "         10  100  1005  1006  1007  1009  101  1014  1016  1022  ...  962  \\\n0  0.000000  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n1  0.000000  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n2  0.090374  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n3  0.018773  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n4  0.130861  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n\n   965  968  970  973  977   98  980  987  998  \n0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n\n[5 rows x 3869 columns]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df_text = pd.DataFrame(denselist, columns=feature_names)\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>10</th>\n      <th>100</th>\n      <th>1005</th>\n      <th>1006</th>\n      <th>1007</th>\n      <th>1009</th>\n      <th>101</th>\n      <th>1014</th>\n      <th>1016</th>\n      <th>1022</th>\n      <th>...</th>\n      <th>973</th>\n      <th>977</th>\n      <th>98</th>\n      <th>980</th>\n      <th>987</th>\n      <th>998</th>\n      <th>target authors</th>\n      <th>venue</th>\n      <th>coauthors</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[42, 36]</td>\n      <td>0.043011</td>\n      <td>[13720]</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[45]</td>\n      <td>0.004301</td>\n      <td>[1359, 15881]</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.090374</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[97]</td>\n      <td>0.008602</td>\n      <td>[]</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.018773</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[2]</td>\n      <td>0.019355</td>\n      <td>[19617]</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.130861</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[44, 2]</td>\n      <td>0.000000</td>\n      <td>[9641, 5623]</td>\n      <td>18</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 3873 columns</p>\n</div>",
      "text/plain": "         10  100  1005  1006  1007  1009  101  1014  1016  1022  ...  973  \\\n0  0.000000  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n1  0.000000  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n2  0.090374  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n3  0.018773  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n4  0.130861  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n\n   977   98  980  987  998  target authors     venue      coauthors  year  \n0  0.0  0.0  0.0  0.0  0.0        [42, 36]  0.043011        [13720]     9  \n1  0.0  0.0  0.0  0.0  0.0            [45]  0.004301  [1359, 15881]    15  \n2  0.0  0.0  0.0  0.0  0.0            [97]  0.008602             []    10  \n3  0.0  0.0  0.0  0.0  0.0             [2]  0.019355        [19617]    10  \n4  0.0  0.0  0.0  0.0  0.0         [44, 2]  0.000000   [9641, 5623]    18  \n\n[5 rows x 3873 columns]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_auth_venue = df[[\"target authors\", \"venue\", \"coauthors\", \"year\"]]\n",
    "df_full       = pd.concat([df_text.reset_index(drop=True), df_auth_venue.reset_index(drop=True)], axis=1)\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Preprocessing\n",
    "\n",
    "We previously used a binning strategy of dealing with coauthors. This time, we instead use a simple count of how many coauthors (with `id>100`) they have previously colaborated with appear in the paper. To do this, we build a dictionary of lists of coauthors for each prolific author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_collaborator_dictionary(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Constructs a database of collobaroter for given author id key. \n",
    "    \"\"\"\n",
    "    \n",
    "    collaboraters = {}\n",
    "    authors       = np.arange(100)\n",
    "    \n",
    "    for author in authors:\n",
    "        df_auth = df.copy(deep=True)\n",
    "        df_auth[\"label\"] = df_auth[\"target authors\"].apply(lambda x: 1 if author in x else 0)\n",
    "        df_auth = df_auth[df_auth[\"label\"] == 1]\n",
    "        coauths = list(set(df_auth[\"coauthors\"].sum()))\n",
    "        collaboraters[author] = coauths\n",
    "    \n",
    "    return collaboraters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborator_db = construct_collaborator_dictionary(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "\n",
    "We now build and validate a model. We use a `RANDOM_STATE` seed to ensure we generate the same training/evaluation split. We write our results to a csv file to avoid an unecessarily long notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_training(X_train, y_train):\n",
    "    \"\"\"\n",
    "    upsamples the minority class until class balance is achieved\n",
    "    \"\"\"\n",
    "    X = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    \n",
    "    pos = X[X[\"label\"] == 1]\n",
    "    neg = X[X[\"label\"] == 0]\n",
    "    \n",
    "    pos_upsample = resample(pos, replace=True, n_samples=len(neg), random_state=RANDOM_STATE)\n",
    "    \n",
    "    resampled = pd.concat([neg, pos_upsample])\n",
    "\n",
    "    y_train = resampled[\"label\"]\n",
    "    X_train = resampled.drop([\"label\"], axis=1)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def resample_training(X_train, y_train):\n",
    "    \"\"\"\n",
    "    resamples class imbalance using SMOTE: \n",
    "    https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
    "    \"\"\"\n",
    "    sm = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_data(author: int, df:pd.DataFrame):\n",
    "    # take copy and prepare label\n",
    "    df = df.copy(deep=True)\n",
    "    df[\"label\"] = df[\"target authors\"].apply(lambda x: 1 if author in x else 0)\n",
    "    \n",
    "    # map number of collaborators for this given instance \n",
    "    collabs = collaborator_db[author]\n",
    "    df[\"num collaborators\"] = df[\"coauthors\"].apply(lambda x: len(set(x).intersection(collabs)))\n",
    "    \n",
    "    # drop irrelevant columns\n",
    "    X = df.drop([\"label\", \"target authors\", \"coauthors\", \"year\"], axis=1)\n",
    "    y = df[\"label\"]\n",
    "    \n",
    "    # split training and validation - we have fixed random state for reproducability\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # upsample to deal with class imbalance\n",
    "    X_train, y_train = upsample_training(X_train, y_train)\n",
    "    \n",
    "    # we convert to numpy arrays for fitting to sklearn models\n",
    "    # the reason for this is that sklearn throws annoying warnings otherwise\n",
    "    return np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_to_csv(df: pd.DataFrame):#, restr_lst):\n",
    "    \"\"\"\n",
    "    As we are building 100 classifiers, printing f1 scores within a notebook is impractical. \n",
    "    following function writes results to csv. \n",
    "    \"\"\"\n",
    "    author_lst = []\n",
    "    with open(\"Results/validation_yearLogistic.csv\", mode='w') as f:    \n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        header = ['Author Id','F1 score']\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        # loop over each author, build classifier and write to output\n",
    "        authors = np.arange(100)\n",
    "        \n",
    "        avg = 0\n",
    "        for author in tqdm(authors):\n",
    "\n",
    "            X_train, X_val, y_train, y_val = get_train_val_data(author, df)\n",
    "            \n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred = clf.predict(X_val)\n",
    "            \n",
    "            f1 = f1_score(y_pred, y_val)\n",
    "            avg += f1\n",
    "            writer.writerow([author, f1])\n",
    "\n",
    "            #To see the changes to the low-F1 authors after restricting on year\n",
    "            if f1 <= 0.5:\n",
    "                author_lst.append(author)\n",
    "\n",
    "        writer.writerow([\"average\", avg])\n",
    "    return author_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>10</th>\n      <th>100</th>\n      <th>1005</th>\n      <th>1006</th>\n      <th>1007</th>\n      <th>1009</th>\n      <th>101</th>\n      <th>1014</th>\n      <th>1016</th>\n      <th>1022</th>\n      <th>...</th>\n      <th>973</th>\n      <th>977</th>\n      <th>98</th>\n      <th>980</th>\n      <th>987</th>\n      <th>998</th>\n      <th>target authors</th>\n      <th>venue</th>\n      <th>coauthors</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[42, 36]</td>\n      <td>0.043011</td>\n      <td>[13720]</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[45]</td>\n      <td>0.004301</td>\n      <td>[1359, 15881]</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.090374</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[97]</td>\n      <td>0.008602</td>\n      <td>[]</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.018773</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[2]</td>\n      <td>0.019355</td>\n      <td>[19617]</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.130861</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[44, 2]</td>\n      <td>0.000000</td>\n      <td>[9641, 5623]</td>\n      <td>18</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 3873 columns</p>\n</div>",
      "text/plain": "         10  100  1005  1006  1007  1009  101  1014  1016  1022  ...  973  \\\n0  0.000000  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n1  0.000000  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n2  0.090374  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n3  0.018773  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n4  0.130861  0.0   0.0   0.0   0.0   0.0  0.0   0.0   0.0   0.0  ...  0.0   \n\n   977   98  980  987  998  target authors     venue      coauthors  year  \n0  0.0  0.0  0.0  0.0  0.0        [42, 36]  0.043011        [13720]     9  \n1  0.0  0.0  0.0  0.0  0.0            [45]  0.004301  [1359, 15881]    15  \n2  0.0  0.0  0.0  0.0  0.0            [97]  0.008602             []    10  \n3  0.0  0.0  0.0  0.0  0.0             [2]  0.019355        [19617]    10  \n4  0.0  0.0  0.0  0.0  0.0         [44, 2]  0.000000   [9641, 5623]    18  \n\n[5 rows x 3873 columns]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_function(author, df):\n",
    "    #for item in author_lst:\n",
    "    # take copy and prepare label\n",
    "    df = df.copy(deep=True)\n",
    "    df[\"label\"] = df[\"target authors\"].apply(lambda x: 1 if author in x else 0)\n",
    "    df_val = df[(df[\"label\"] == 1)] \n",
    "    df_plot = df[(df[\"label\"] == 1) & (df[\"year\"] >= 9)] \n",
    "    print(\"length\", len(df_plot))\n",
    "    print(\"length val\", len(df_val))\n",
    "\n",
    "    #Then plot based on those the author is in\n",
    "    plt.hist(df_val[\"year\"], bins=np.arange(df_val[\"year\"].min(), df_val[\"year\"].max()))\n",
    "    return\n",
    "\n",
    "def restrict_function(restr_lst, df):\n",
    "    #for item in author_lst:\n",
    "    # take copy and prepare label\n",
    "    for author in restr_lst:\n",
    "        df[\"label\"] = df[\"target authors\"].apply(lambda x: 1 if author in x else 0)\n",
    "        df = df.drop(df[(df[\"year\"] < 13) & (df[\"label\"] == 1)].index)\n",
    "        df = df.drop([\"label\"], axis=1)\n",
    "    return df\n",
    "\n",
    "#From using ModelOriginal.ipynb with logistic regression the following authors are seen to have an average F1 of at most 0.5:\n",
    "restr_lst = [71, 18, 28, 86, 62, 3, 81, 51, 34, 6, 20, 99] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_function(restr_lst[0], df_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:30<00:00,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 13, 17, 18, 38, 48, 71, 80, 86, 89, 91, 99]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_restr = restrict_function(restr_lst, df_full)\n",
    "\n",
    "result = validate_to_csv(df_restr)\n",
    "print(result)\n",
    "#[(0.3333333333333333, 71), (0.34782608695652173, 18), (0.3529411764705882, 28), \n",
    "# (0.3529411764705882, 86), (0.3555555555555555, 62), (0.3846153846153846, 3), \n",
    "# (0.4, 81), (0.4761904761904762, 51), (0.48484848484848486, 34), (0.5, 6), (0.5, 20), (0.5, 99)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Binary Classifiers\n",
    "\n",
    "We now build the 100 binary classifers, one for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:37<00:00,  4.57s/it]\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "authors = np.arange(100)\n",
    "\n",
    "for author in tqdm(authors):\n",
    "            \n",
    "    X_train, X_val, y_train, y_val = get_train_val_data(author, df_restr)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Predictions\n",
    "\n",
    "We now load in our `test.json` set to generate predictions. We first put the training data through the usual preprocessing pipeline and exclude vocabular that was not part of our `tf-idf` training lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 800 instances\n"
     ]
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>identifier</th>\n      <th>coauthors</th>\n      <th>year</th>\n      <th>venue</th>\n      <th>text</th>\n      <th>str text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[16336, 1762, 4357, 12564]</td>\n      <td>19</td>\n      <td>0.479570</td>\n      <td>[3207, 24, 1798, 1738, 37, 2375, 1568, 11, 53,...</td>\n      <td>3207 24 1798 1738 37 2375 1568 11 53 1584 1903...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[21189, 14088]</td>\n      <td>19</td>\n      <td>0.479570</td>\n      <td>[40, 1560, 1536, 1544, 1609, 1705, 1658, 1543,...</td>\n      <td>40 1560 1536 1544 1609 1705 1658 1543 52 11 33...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[3625, 1198, 19889, 794, 2749, 7801]</td>\n      <td>19</td>\n      <td>0.015054</td>\n      <td>[47, 1574, 1729, 1641, 11, 37, 2533, 2015, 47,...</td>\n      <td>47 1574 1729 1641 11 37 2533 2015 47 1930 1549...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[19810, 15173, 5876, 111]</td>\n      <td>19</td>\n      <td>0.045161</td>\n      <td>[1770, 53, 2054, 1549, 1529, 1723, 2796, 1547,...</td>\n      <td>1770 53 2054 1549 1529 1723 2796 1547 1543 47 ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[10932, 7668, 11907, 19601, 15307, 10492, 1049...</td>\n      <td>19</td>\n      <td>1.000000</td>\n      <td>[18, 1924, 23, 1544, 3927, 2686, 1543, 1535, 1...</td>\n      <td>18 1924 23 1544 3927 2686 1543 1535 1660 1548 ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   identifier                                          coauthors  year  \\\n0           0                         [16336, 1762, 4357, 12564]    19   \n1           1                                     [21189, 14088]    19   \n2           2               [3625, 1198, 19889, 794, 2749, 7801]    19   \n3           3                          [19810, 15173, 5876, 111]    19   \n4           4  [10932, 7668, 11907, 19601, 15307, 10492, 1049...    19   \n\n      venue                                               text  \\\n0  0.479570  [3207, 24, 1798, 1738, 37, 2375, 1568, 11, 53,...   \n1  0.479570  [40, 1560, 1536, 1544, 1609, 1705, 1658, 1543,...   \n2  0.015054  [47, 1574, 1729, 1641, 11, 37, 2533, 2015, 47,...   \n3  0.045161  [1770, 53, 2054, 1549, 1529, 1723, 2796, 1547,...   \n4  1.000000  [18, 1924, 23, 1544, 3927, 2686, 1543, 1535, 1...   \n\n                                            str text  \n0  3207 24 1798 1738 37 2375 1568 11 53 1584 1903...  \n1  40 1560 1536 1544 1609 1705 1658 1543 52 11 33...  \n2  47 1574 1729 1641 11 37 2533 2015 47 1930 1549...  \n3  1770 53 2054 1549 1529 1723 2796 1547 1543 47 ...  \n4  18 1924 23 1544 3927 2686 1543 1535 1660 1548 ...  "
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"test.json\"\n",
    "df_test = load_data_set(path)\n",
    "df_test = preprocess(df_test, train=False)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this complicated looking lambda simply removes any words that were not part of our preprocessing. \n",
    "# failing to do so, would pass an unseen word to our tf-idf vectoriser and would crash our program\n",
    "tfidf_features  = tfidf.get_feature_names_out()\n",
    "df_test['text'] = df_test['text'].apply(lambda xs: list(filter((lambda x: str(x) in tfidf_features), xs)))\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda xs: ''.join(str(x)+' ' for x in xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now apply the tf-idf transformation to the text component\n",
    "X_test = tfidf.transform(df_test['text'])\n",
    "X_test = pd.DataFrame((X_test.todense().tolist()), columns=tfidf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now put everything back together\n",
    "test = pd.concat([X_test, df_test[\"venue\"], df_test[\"coauthors\"], df_test[\"year\"]], axis=1)\n",
    "#plt.hist(test[\"year\"], bins=np.arange(test[\"year\"].min(), test[\"year\"].max()))\n",
    "#test = test.drop([\"year\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    function for writing predictions to output file. \n",
    "    WARNING: Deletes predictions.csv if present in working directory\n",
    "    \"\"\"\n",
    "    if os.path.exists(\"Results/predictionsyearLogistic.csv\"):\n",
    "        os.remove(\"Results/predictionsyearLogistic.csv\")\n",
    "        print(\"removed previous predictions\")\n",
    "    \n",
    "    \n",
    "    with open(\"Results/predictionsyearLogistic.csv\", mode='w') as f:    \n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        header = ['Id','Predict']\n",
    "        writer.writerow(header)\n",
    "        n      = X_test.shape[0]\n",
    "        \n",
    "        # loop over each training sample and write to necessary format\n",
    "        for Id in tqdm(range(n)):\n",
    "            \n",
    "            # we need to keep x as a dataframe for this model so we can apply the collobartor mapping easily\n",
    "            x   = test_df.iloc[Id].to_frame().T\n",
    "            row = [Id]\n",
    "            authors = \"\"\n",
    "            \n",
    "            for author, model in enumerate(models):\n",
    "                # map number of collaborators for this given instance \n",
    "                X = x.copy(deep = True)\n",
    "                collabs = collaborator_db[author]\n",
    "                X[\"num collaborators\"] = X[\"coauthors\"].apply(lambda x: len(set(x).intersection(collabs)))\n",
    "                X = X.drop([\"coauthors\"], axis=1)\n",
    "                X = np.array(X).reshape(1, -1)\n",
    "               \n",
    "                if np.array(model.predict(X)).item() == 1:\n",
    "                    authors += str(author) + \" \"\n",
    "\n",
    "            # to match the output requirement \n",
    "            if len(authors) == 0: row = [Id, -1]\n",
    "            else: row = [Id, authors]\n",
    "            \n",
    "            writer.writerow(row)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [02:02<00:00,  6.55it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = test.drop([\"year\"], axis=1)\n",
    "make_predictions(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "7102"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_restr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "7460"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "7460"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3812jvsc74a57bd0e245b9d4d52625933425f13c940396e11f2ad0cf135519173d3aca2cac5d4603"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}