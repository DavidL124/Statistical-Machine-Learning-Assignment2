{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741339a4-47db-44b9-b374-5216b66e8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy, os, csv\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "from collections import defaultdict as dd\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657bf204-8e9f-40ed-b953-170c0992a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct an RNN network.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9af04e-6d43-4d56-a510-4b2c6b93625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: Work in progress: do not run code yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2092e9-3807-4deb-9497-312e52ddfc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f738fbe7-9655-44af-ad1d-cdbb7e91fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read into a dataframe.\n",
    "\n",
    "def load_data_set(path: str):\n",
    "    \"\"\"\n",
    "    loads data set located at path and returns as pandas data frame\n",
    "    \"\"\"\n",
    "    with open(path) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    print(f\"loaded {len(data)} instances\")\n",
    "    data = pd.json_normalize(data)  #Normalize semi-structured JSON data into a flat table.\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba86a19-6d96-4606-9d50-c6731ce8fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformations\n",
    "\n",
    "def filter_authors(authors: List[int], prolifics=True):\n",
    "    \n",
    "    if prolifics:\n",
    "        prolifics = filter(lambda x: x < 100, authors)\n",
    "        return list(prolifics)\n",
    "    \n",
    "    else:\n",
    "        coauthors = filter(lambda x: x>=100, authors)\n",
    "        return list(coauthors)\n",
    "    \n",
    "    \n",
    "def text_to_vector(text: List[int]):\n",
    "    \"\"\"\n",
    "    Converts text to sparse matrix representation\n",
    "    text: List of integers between 1, 4999\n",
    "    \"\"\"\n",
    "    \n",
    "    word_vec = np.zeros(5000, dtype=int)\n",
    "    \n",
    "    for word in text:\n",
    "        word_vec[word] += 1 \n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def statistics_feature(lst: List[int], length=True):\n",
    "    if length:\n",
    "        return len(lst)\n",
    "    else:\n",
    "        if len(lst) <= 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return statistics.stdev(lst)\n",
    "\n",
    "def pre_processing(df, train=True):\n",
    "    \n",
    "    if train:\n",
    "        df[\"target authors\"] = df[\"authors\"].apply(lambda x: filter_authors(x))\n",
    "        df[\"coauthors\"]      = df[\"authors\"].apply(lambda x: filter_authors(x, prolifics=False))\n",
    "        df = df.drop([\"authors\"], axis=1)\n",
    "    \n",
    "    ########### preprocessing for text\n",
    "    df[\"combined text\"] = df[\"abstract\"]# + df[\"title\"] #+ df[\"abstract\"]\n",
    "    df[\"word_mean\"] = df[\"combined text\"].apply(lambda x: np.mean(x))  \n",
    "    df[\"word_count\"] = df[\"combined text\"].apply(lambda x: statistics_feature(x))\n",
    "    df[\"word_spread\"] = df[\"combined text\"].apply(lambda x: statistics_feature(x, length=False))\n",
    "    #df = df.drop([\"combined text\"], axis=1)\n",
    "    ###########\n",
    "    \n",
    "    df[\"abstract\"] = df[\"abstract\"].apply(lambda x: text_to_vector(x))\n",
    "    df[\"title\"]    = df[\"title\"].apply(lambda x: text_to_vector(x))\n",
    "    df[\"text\"]     = df[\"title\"] + df[\"abstract\"]\n",
    "    text_df = pd.DataFrame(df.text.tolist(), index=df.index)\n",
    "    \n",
    "    #preprocessing for venue. We use minmax scaling as a matter of best-practice. \n",
    "    # as we require all rows to have integer values, we give blank venues a dummy value of 465\n",
    "    scalar = MinMaxScaler()\n",
    "    df.loc[df.venue == \"\", \"venue\"] = 465\n",
    "    df[\"venue\"] = scalar.fit_transform(df[\"venue\"].to_numpy().reshape(-1, 1))\n",
    "    ##Repeat this for the word_mean, count and spread\n",
    "    df[\"word_mean\"] = scalar.fit_transform(df[\"word_mean\"].to_numpy().reshape(-1, 1))\n",
    "    df[\"word_spread\"] = scalar.fit_transform(df[\"word_spread\"].to_numpy().reshape(-1, 1))\n",
    "    df[\"word_count\"] = scalar.fit_transform(df[\"word_count\"].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    # prepocessing for coauthors\n",
    "    # we use a discretised binning strategy, with n=10 bins by default. \n",
    "    df[\"coauthors\"] = df[\"coauthors\"].apply(lambda x: build_bins(x, n_bins=10))\n",
    "    coauth_df = pd.DataFrame(df.coauthors.tolist(), index=df.index)\n",
    "    \n",
    "    # dropping irrelevant columns & concat with 5000-column text_df\n",
    "    df = df.drop([\"abstract\", \"title\", \"text\", \"year\", \"coauthors\"], axis=1)\n",
    "    #df = df.drop([\"year\", \"coauthors\"], axis=1) #So we use either the abstract, title or text for the encoding.\n",
    "    \n",
    "    df = pd.concat([df, text_df, coauth_df], axis=1)\n",
    "    \n",
    "    # and drop row identifier if test set\n",
    "    if not train:\n",
    "        df = df.drop([\"identifier\"], axis=1)\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "def build_bins(coauthors: List[int], n_bins=10):\n",
    "    \"\"\"\n",
    "    takes a list of coauthors and returns 10-column data frame\n",
    "    \n",
    "    This might be some of the uggliest code I have ever written, though\n",
    "    sklearn's discrete bins didn't really give what I wanted\n",
    "    \"\"\"\n",
    "    width = np.ceil(21246/n_bins)\n",
    "    bins  = np.zeros(n_bins)\n",
    "    for author in coauthors:\n",
    "        i = 0\n",
    "        while not (max(0,(i-1))*width <= author <= i*width):\n",
    "            i += 1\n",
    "        bins[i-1] += 1\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ace913-2b36-4865-93ca-8655030afc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst = [0, 37, 38, 16, 22, 17, 30, 19, 15, 18, 17, 0, 16, 17, 30, 16, 41, 15, 26, 26, 16, 33, 34, 18, 1, 34, 30, 0, 18, 0, 18, 16, 0, 15, 32, 15, 49, 15, 24, 17, 16, 34, 39, 0, 0, 46, 17, 19, 18, 17, 18, 16, 28, 15, 19, 20, 22, 17, 41, 0, 0, 25, \n",
    "#16, 21, 18, 15, 21, 22, 17, 30, 15, 19, 17, 27, 15, 17, 28, 0, 0, 44, 15, 21, 17, 21, 15, 19, 17, 18, 41, 19, 0, 38, 17, \n",
    "#18, 17, 24, 19, 15, 19, 15, 15, 36, 24, 18, 0, 0, 17, 16, 27, 21, 18, 25, 15, 0, 16, 0, 41, 15, 17, 32, 28, 0, 16, 16, 16, \n",
    "#15, 27, 27, 15, 17, 17, 28, 0]\n",
    "#len(tst) #133\n",
    "#print(statistics.mean([1,3,4,5,101]))\n",
    "#int(statistics.mean([1,3,4,5,101]))\n",
    "#7%5\n",
    "#lst1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "#lst1[(len(lst1)-len(lst1)%5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7126f97-9658-4dfb-922b-6710c9500d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct tensor vectors for authors (based on Week 9 Workshop code)\n",
    "all_categories = [0, 1]\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size) # changed input\n",
    "        self.activation = nn.Tanh() # new\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.activation(self.i2h(combined)) # changed to use activation\n",
    "        output = self.h2o(hidden) # changed input\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "def simplify_num(lst, type_col = \"abstract\"):\n",
    "    #Round to nearest 100.\n",
    "    lst1 = []\n",
    "    #for item in lst:\n",
    "    for i in range(len(lst)):\n",
    "        temp = int(round(lst[i], -2)/100)\n",
    "        if temp == 50:\n",
    "            lst[i] = 49\n",
    "        else:\n",
    "            lst[i] = temp\n",
    "            \n",
    "    #Depending on the type of the column make reductions.\n",
    "    if type_col == \"title\":\n",
    "        for item in lst:\n",
    "            lst1.append(item)\n",
    "        return lst1\n",
    "    \n",
    "    elif type_col != \"title\":\n",
    "        lst_len = len(lst)\n",
    "        for j in range(len(lst)):\n",
    "            if (1+j) % 5 == 0: #if (1+j) % 5 == 0:\n",
    "                lst1.append(int(statistics.mean(lst[j-4:j]))) \n",
    "                #lst1.append(int(statistics.mean(lst[j-4:j])))\n",
    "                \n",
    "        #Append average of any remaining values\n",
    "        if lst_len % 5 != 0: #if lst_len % 5 != 0:\n",
    "            lst1.append(int(statistics.mean(lst[(lst_len-(lst_len%5)):])))\n",
    "            #lst1.append(int(statistics.mean(lst[(lst_len-(lst_len%5)):])))\n",
    "        return lst1 \n",
    "    \n",
    "    return lst\n",
    "\n",
    "######From the Week 9 Tutorial:               \n",
    "def lineToTensor(lst_init): #, index_lst: int):\n",
    "    n_letters = 50\n",
    "    #lst_init = df.loc[index_lst, 'combined text']\n",
    "    #print(\"lst before\", lst_init)\n",
    "    lst = simplify_num(lst_init)\n",
    "    #print(\"lst\", lst)\n",
    "    tensor = torch.zeros(len(lst), 1, n_letters)\n",
    "    for li, num in enumerate(lst):\n",
    "        #print(\"li =\", li, \"num =\", num)\n",
    "        tensor[li][0][num] = 1\n",
    "    return tensor\n",
    "\n",
    "###### For testing the model.\n",
    "\n",
    "def constructSample(df, index_lst: int):\n",
    "    \n",
    "    n_val = 50\n",
    "    #print(df.head())\n",
    "    #print(df.loc[index_lst, 'test_text'])\n",
    "    lst_init = df.loc[index_lst, 'combined text']\n",
    "    lst = simplify_num(lst_init)\n",
    "    \n",
    "    label = df.loc[index_lst, 'label']\n",
    "    #print(\"Before initialisation\")\n",
    "    #Initiate the tensor.\n",
    "    #for li, letter in \n",
    "    tensor = torch.zeros(len(lst), 1, n_val)\n",
    "    #print(\"After initialisation\")\n",
    "    \n",
    "    #Construct the category tensor.\n",
    "    #print(\"all_categories\", all_categories)\n",
    "    #print(\"label\", label)\n",
    "    label_tensor = torch.tensor([all_categories.index(label)], dtype=torch.long)\n",
    "    #if index_lst < 5:\n",
    "    #    print(lst)\n",
    "    #    print(\"label_tensor\", label_tensor)\n",
    "    for i, num in enumerate(lst):\n",
    "        \n",
    "        tensor[i][0][num] = 1\n",
    "        \n",
    "    return label, lst, label_tensor, tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dad2fb9c-f70b-4665-ac4c-a0b2a198b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_categories = [0,1]\n",
    "\n",
    "def evaluate_RNN(rnn, line_tensor):\n",
    "    ##Adapted from \"NLP From Scratch\".\n",
    "    hidden = rnn.initHidden()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "        \n",
    "    return output\n",
    "\n",
    "##use this function on User Input.\n",
    "def predict_output(rnn, input_line, Id, n_predictions = 2):\n",
    "    \n",
    "    n_val = 50\n",
    "    all_categories = [0,1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #Obtain the tensor representation of the input.\n",
    "        #using the lineToTensor function from the Week 9 Tutorial:\n",
    "        output = evaluate_RNN(rnn, lineToTensor(input_line))#, Id))\n",
    "        \n",
    "        #Get top 2 categories (only 2 categories here):\n",
    "        #Get the top N categories.\n",
    "        #top_k, top_ind = output.topk(n_predictions, 1, True)\n",
    "        \n",
    "        top_1, top_ind = output.topk(1)\n",
    "        category_index = top_ind[0].item()\n",
    "        \n",
    "        guess, guess_i = all_categories[category_index], category_index\n",
    "        #for i in range(n_predictions):\n",
    "        #    value = top_k[0][i].item()\n",
    "        #    category_index = top_ind[0][i].item()\n",
    "        #    predictions.append([value, all_categories[category_index]])\n",
    "            \n",
    "    return guess #predictions[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d448b6e-f211-4069-a73c-8b4d3135f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now train the data.\n",
    "# training\n",
    "'''\n",
    "def train_RNN_classifier(category_tensor, lst_tensor):\n",
    "    ###################use the tensor function here. Work in progress...\n",
    "    ####Adapted from Week 9 Tutorial\n",
    "    n_hidden = 32\n",
    "    n_instances = len(df)\n",
    "    n_val = 50\n",
    "    learning_rate = 0.005\n",
    "    no_catgories = 2\n",
    "    \n",
    "    rnn = RNNClassifier(n_val, n_hidden, no_catgories)\n",
    "    criterion = nn.NLLLoss()\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    for i in range(lst_tensor.size()[0]):\n",
    "        output, hidden = rnn(lst_tensor[i], hidden)\n",
    "        \n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    #Add parameters' gradients to their values, multiplied by the learning rate.\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "        \n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "def run_training_function(author: int, df, n_instances=5000):\n",
    "    \n",
    "    # Set up copy and construct the label for RNN training.\n",
    "    #print(\"Copy 1\")\n",
    "    df[\"label\"] = df[\"target authors\"].apply(lambda x: 1 if author in x else 0)   \n",
    "    df = df.drop(['target authors'], axis=1)\n",
    "    #print(\"Copy 2\")\n",
    "    \n",
    "    #Trains the RNN function: adapted from Week 9 Tutorial.\n",
    "    \n",
    "    print_every=2000\n",
    "    correct = 0\n",
    "    #set n_iters as 5000 first.\n",
    "    n_instances = len(df)\n",
    "    current_loss = 0\n",
    "    \n",
    "    for i in tqdm(range(n_instances)):\n",
    "        \n",
    "        category, lst, category_tensor, lst_tensor = constructSample(df, i)\n",
    "        \n",
    "        ##Then start the training process.\n",
    "        \n",
    "        output, loss = train_RNN_classifier(category_tensor, lst_tensor)\n",
    "        current_loss += loss\n",
    "        #output, h_n = nn.RNN(category_tensor)\n",
    "        \n",
    "        #Get the top category. The \"CategoryFromOutput\" function from the Week 9 lab is included here\n",
    "        top_1, top_ind = output.topk(1)\n",
    "        category_index = top_ind[0].item()\n",
    "        #print(\"category_index\", category_index)\n",
    "        ##make a guess\n",
    "        guess, guess_i = all_categories[category_index], category_index\n",
    "            \n",
    "        if guess == category: \n",
    "            correct += 1\n",
    "            \n",
    "        #Print every:\n",
    "        if i % print_every == 0: \n",
    "            print('%d %s %s' % (i, guess, category))\n",
    "    #Examine the accuracy.\n",
    "    df = df.drop(['label'], axis=1)\n",
    "    df = df.drop(['combined text'], axis = 1)\n",
    "    print(\"Accuracy is: %.4f\" %(correct/n_instances))\n",
    "'''\n",
    "            \n",
    "\n",
    "def train_classifier(author: int, df: pd.DataFrame, debug=False):\n",
    "    \"\"\"\n",
    "    Trains a classifier for author i. Assumes text-vectorisaiton has occured.\n",
    "    \n",
    "    Model Features:\n",
    "    text vectorisation\n",
    "    \"\"\"\n",
    "    # create copy and set up label\n",
    "    df = df.copy(deep=True)\n",
    "    df[\"label\"] = df[\"target authors\"].apply(lambda x: 1 if author in x else 0)\n",
    "    \n",
    "    df = df.drop(['target authors', 'text', 'abstract', 'title'], axis=1)  ##This provides the input for the category tensor!\n",
    "    #############ideally now drop text, abstract and text \n",
    "    \n",
    "    # split up positive and negative instances so as to ensure a balanced training set \n",
    "    # if we don't do this, we end up with a very imbalanced training set \n",
    "    # however, if we don't include enough negative samples, we tend to \"overclassify\". \n",
    "    # we can tune out performance with the below 'neg sample factor'\n",
    "    \n",
    "    neg_sample_factor = 10\n",
    "    \n",
    "    pos = df[df['label'] == 1] \n",
    "    neg = df[df['label'] == 0]\n",
    "    \n",
    "    n_pos_samples = pos.shape[0]\n",
    "    n_tot_samples = df.shape[0]\n",
    "    \n",
    "    # takes a sample of the negative instances to train on\n",
    "    neg = neg.sample(frac=neg_sample_factor*(n_pos_samples/n_tot_samples), random_state=51) \n",
    "    \n",
    "    if debug:\n",
    "        print(f\"training on {pos.shape[0]} postitive instances\")\n",
    "        print(f\"training on {neg.shape[0]} negative  instances\")\n",
    "    \n",
    "    df = pd.concat([pos, neg])\n",
    "    X_train = df.loc[:, df.columns != \"label\"]\n",
    "    y_train = df[\"label\"]\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"training on {X_train.shape[0]} instances\")\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    if debug:\n",
    "        y_train_pred = clf.predict(X_train) \n",
    "        acc = accuracy_score(y_train, y_train_pred) \n",
    "        f1  = f1_score(y_train, y_train_pred)\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "        print(f\"f1 score: {f1}\")\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5630ce56-152e-4065-ae39-241174fb6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################## validation output\n",
    "'''\n",
    "def train_RNN_classifier(category_tensor, lst_tensor, rnn):\n",
    "    ###################use the tensor function here. Work in progress...\n",
    "    ####Adapted from Week 9 Tutorial\n",
    "    n_instances = len(df)\n",
    "    learning_rate = 0.005\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    for i in range(lst_tensor.size()[0]):\n",
    "        output, hidden = rnn(lst_tensor[i], hidden)\n",
    "        \n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    #Add parameters' gradients to their values, multiplied by the learning rate.\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "        \n",
    "    return output, loss.item()\n",
    "'''\n",
    "\n",
    "\n",
    "def run_training_function(author: int, df, n_instances=1000):\n",
    "    \n",
    "\n",
    "    n_hidden = 32\n",
    "    n_val = 50\n",
    "    no_catgories = 2\n",
    "    rnn = RNNClassifier(n_val, n_hidden, no_catgories)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # Set up copy and construct the label for RNN training.\n",
    "    #print(df.head())\n",
    "    df[\"label\"] = df[\"target authors\"].apply(lambda x: 1 if author in x else 0)  \n",
    "    #df = df.drop(['target authors'], axis=1)\n",
    "    \n",
    "    #Trains the RNN function: adapted from Week 9 Tutorial.\n",
    "    \n",
    "    print_every=2000\n",
    "    correct = 0\n",
    "    #n_instances = len(df)\n",
    "    current_loss = 0\n",
    "    \n",
    "    def train_RNN_classifier(category_tensor, lst_tensor):\n",
    "        ###################use the tensor function here. Work in progress...\n",
    "        ####Adapted from Week 9 Tutorial\n",
    "        n_instances = len(df)\n",
    "        learning_rate = 0.005\n",
    "        hidden = rnn.initHidden()\n",
    "        rnn.zero_grad()\n",
    "    \n",
    "        for i in range(lst_tensor.size()[0]):\n",
    "            output, hidden = rnn(lst_tensor[i], hidden)\n",
    "        \n",
    "        loss = criterion(output, category_tensor)\n",
    "        loss.backward()\n",
    "    \n",
    "        #Add parameters' gradients to their values, multiplied by the learning rate.\n",
    "        for p in rnn.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "        \n",
    "        return output, loss.item()\n",
    "    \n",
    "    for i in range(n_instances):\n",
    "        \n",
    "        category, lst, category_tensor, lst_tensor = constructSample(df, i)\n",
    "        \n",
    "        ##Then start the training process.\n",
    "        \n",
    "        output, loss = train_RNN_classifier(category_tensor, lst_tensor)\n",
    "        current_loss += loss\n",
    "        #output, h_n = nn.RNN(category_tensor)\n",
    "        \n",
    "        #Get the top category. The \"CategoryFromOutput\" function from the Week 9 lab is included here\n",
    "        top_1, top_ind = output.topk(1)\n",
    "        category_index = top_ind[0].item()\n",
    "        #print(\"category_index\", category_index)\n",
    "        ##make a guess\n",
    "        guess, guess_i = all_categories[category_index], category_index\n",
    "            \n",
    "        if guess == category: \n",
    "            correct += 1\n",
    "            \n",
    "        #Print every:\n",
    "        #if i % print_every == 0: \n",
    "        #    print('%d %s %s' % (i, guess, category))\n",
    "    #Examine the accuracy.\n",
    "    df = df.drop(['label'], axis=1)\n",
    "    #df = df.drop(['combined text'], axis = 1)\n",
    "    #print(\"Accuracy is: %.4f\" %(correct/n_instances))\n",
    "    return rnn\n",
    "    \n",
    "########################################################################## validation output\n",
    "def test_RNN_model(rnn_lst, test_df):\n",
    "    #Repeat parts of the test function.\n",
    "    \n",
    "    if os.path.exists(\"predictionsRNN.csv\"):\n",
    "        os.remove(\"predictionsRNN.csv\")\n",
    "        print(\"removed previous RNN predictions\")\n",
    "    \n",
    "    \n",
    "    with open(\"predictionsRNN.csv\", mode='w') as f:    \n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        header = ['Id','Predicted']\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        \n",
    "        X_test = test_df \n",
    "        n      = X_test.shape[0]\n",
    "        #No \"label\" component, as this is the test dataset.\n",
    "        for Id in tqdm(range(n)):\n",
    "            #Obtain the relevant lst value under \"combined text\" combined text\n",
    "            lst = test_df.loc[Id, \"combined text\"]\n",
    "            row = [Id]\n",
    "            authors = []\n",
    "            #Construct the model.\n",
    "            for author, rnn in enumerate(rnn_lst): \n",
    "                output = predict_output(rnn, lst, Id)\n",
    "                if output == 1:\n",
    "                    authors.append(author)\n",
    "            if len(authors) == 0: row.append(-1)\n",
    "            else: row += authors\n",
    "            \n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd5d344-870e-43b4-b275-88ab5ec7cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model validation\n",
    "def validate_model(author: int, df: pd.DataFrame, classifier):\n",
    "    # simple function to assess model performance\n",
    "    \n",
    "    # create copy and set up label\n",
    "    df = df.copy(deep=True)\n",
    "    df[\"label\"] = df[\"target authors\"].apply(lambda x: 1 if author in x else 0)   \n",
    "    df = df.drop(['target authors'], axis=1)\n",
    "    \n",
    "    # split up positive and negative instances so as to ensure a balanced training set \n",
    "    # if we don't do this, we end up with a very imbalanced training set \n",
    "    pos = df[df['label'] == 1] \n",
    "    neg = df[df['label'] == 0]\n",
    "    \n",
    "    # takes a sample of the instances to test on\n",
    "    pos = pos.sample(frac=(1/2))\n",
    "    neg = neg.sample(frac=(1/10))\n",
    "    \n",
    "    # recombine \n",
    "    df = pd.concat([pos, neg])\n",
    "    X_test = df.loc[:, df.columns != \"label\"]\n",
    "    y_test = df[\"label\"]\n",
    "    \n",
    "    # perform predictions (from text only)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_pred, y_test) \n",
    "    f1  = f1_score(y_pred, y_test)#, average=\"samples\") \n",
    "    ##average=\"samples\" is used by the leaderboard (from discussion question #184) but ValueError: Samplewise metrics are not available outside of multilabel classification. \n",
    "    #print(f\"Model {author}\")\n",
    "    #print(f\"Accuracy: {acc}\")\n",
    "    #print(f\"f1 score: {f1}\")\n",
    "    return [acc, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1ba67fa-a7c0-41ea-8934-96f663c50be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_validation_performance(file_name):\n",
    "    \"\"\"\n",
    "    Function for writing validations to output file.\n",
    "    WARNING: Deletes file_name.csv if already present in working directory.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "        print(\"removed previous validations\")\n",
    "    \n",
    "    with open(file_name, mode='w') as f0:\n",
    "        write_valid = csv.writer(f0)\n",
    "        header = ['Id', 'Accuracy', 'F1']\n",
    "        write_valid.writerow(header)\n",
    "        \n",
    "        #Loop over each author.\n",
    "        authors = np.arange(0, 100)\n",
    "        models  = []\n",
    "        sum_accuracy = 0\n",
    "        sum_F1 = 0\n",
    "        \n",
    "        for i in tqdm(authors):\n",
    "            model = train_classifier(i, df, debug=False)\n",
    "            models.append(model)\n",
    "            #Check for performance. Separate function.\n",
    "            return_val = validate_model(i, df, model)\n",
    "            sum_F1 += return_val[1]\n",
    "            sum_accuracy += return_val[0]\n",
    "            write_valid.writerow([i, return_val[0], return_val[1]])\n",
    "            \n",
    "        write_valid.writerow([\"aver\", sum_accuracy/len(authors), sum_F1/len(authors)])\n",
    "        print(f\"Accuracy: {sum_accuracy/len(authors)}\")\n",
    "        print(f\"f1 score: {sum_F1/len(authors)}\")\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01a5c7be-fe38-4c4e-9458-72ec93b7415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test_df: pd.DataFrame, models):\n",
    "    \"\"\"\n",
    "    function for writing predictions to output file. \n",
    "    WARNING: Deletes predictions.csv if present in working directory\n",
    "    \"\"\"\n",
    "    if os.path.exists(\"predictions.csv\"):\n",
    "        os.remove(\"predictions.csv\")\n",
    "        print(\"removed previous predictions\")\n",
    "    \n",
    "    \n",
    "    with open(\"predictions.csv\", mode='w') as f:    \n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        header = ['Id','Predicted']\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        \n",
    "        X_test = test_df \n",
    "        n      = X_test.shape[0]\n",
    "        \n",
    "        # loop over each training sample and write to necessary format\n",
    "        for Id in tqdm(range(n)):\n",
    "            x   = np.array(X_test.iloc[Id]).reshape(1, -1)\n",
    "            row = [Id]\n",
    "            authors = []\n",
    "            #Now iterate through each of the models.\n",
    "            for author, model in enumerate(models):\n",
    "                if np.array(model.predict(x)).item() == 1:\n",
    "                    authors.append(author)\n",
    "\n",
    "            # to match the output requirement \n",
    "            if len(authors) == 0: row.append(-1)\n",
    "            else: row += authors\n",
    "            \n",
    "            writer.writerow(row)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4f0de5f-44bb-4fda-8d38-316f75b1395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def make_RNN_predictions(test_df: pd.DataFrame, models):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2140c9a5-4299-4372-b0bc-2b8316740120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 25793 instances\n"
     ]
    }
   ],
   "source": [
    "#Read into a data frame\n",
    "data_f = pd.read_json(\"train.json\")\n",
    "\n",
    "#Training and validating data\n",
    "path = \"train.json\"\n",
    "df = load_data_set(path)\n",
    "df = pre_processing(df)\n",
    "\n",
    "#file_name = \"validation.csv\"\n",
    "#models = write_validation_performance(file_name)\n",
    "\n",
    "#Building Predictions\n",
    "#path = \"test.json\"\n",
    "#df_test = load_data_set(path)\n",
    "#df_test = pre_processing(df_test, train=False)\n",
    "\n",
    "#make_predictions(df_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb29b087-151c-4c45-b04a-5466c6ff8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing RNN:\n",
    "#print(\"Testing RNN\")\n",
    "#author_id = 42 \n",
    "#run_training_function(author_id, df)\n",
    "\n",
    "#Now for the testing component\n",
    "#make_RNN_predictions(test_df: pd.DataFrame, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfd95747-3a69-4856-8804-f1002f770939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 800 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed previous RNN predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:03<00:00, 256.75it/s]\n"
     ]
    }
   ],
   "source": [
    "#Now for the testing component.\n",
    "path = \"test.json\"\n",
    "df_test = load_data_set(path)\n",
    "df_test = pre_processing(df_test, train=False)\n",
    "\n",
    "rnn_lst = []\n",
    "##Obtain the RNN objects:\n",
    "\n",
    "#authors = np.arange(0, 100)\n",
    "authors = np.arange(0, 1)\n",
    "for i in tqdm(authors):\n",
    "    rnn_lst.append(run_training_function(i, df))\n",
    "\n",
    "test_RNN_model(rnn_lst, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54cf8418-49c1-4ebd-b421-22849298e027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue</th>\n",
       "      <th>target authors</th>\n",
       "      <th>combined text</th>\n",
       "      <th>word_mean</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_spread</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043011</td>\n",
       "      <td>[42, 36]</td>\n",
       "      <td>[25, 19, 23, 15, 18, 19, 20, 29, 15, 19, 21, 1...</td>\n",
       "      <td>0.838297</td>\n",
       "      <td>0.026862</td>\n",
       "      <td>0.472364</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004301</td>\n",
       "      <td>[45]</td>\n",
       "      <td>[0, 15, 17, 24, 15, 36, 22, 19, 16, 15, 15, 20...</td>\n",
       "      <td>0.634257</td>\n",
       "      <td>0.035458</td>\n",
       "      <td>0.598874</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 15, 17, 24, 15, 26, 15, 15, 20, 20, 17, 22...</td>\n",
       "      <td>0.623812</td>\n",
       "      <td>0.028653</td>\n",
       "      <td>0.323598</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008602</td>\n",
       "      <td>[97]</td>\n",
       "      <td>[0, 16, 15, 1, 17, 16, 0, 0, 34, 15, 27, 15, 1...</td>\n",
       "      <td>0.775345</td>\n",
       "      <td>0.032593</td>\n",
       "      <td>0.693761</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019355</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0, 37, 38, 16, 22, 17, 30, 19, 15, 18, 17, 0,...</td>\n",
       "      <td>0.740513</td>\n",
       "      <td>0.043338</td>\n",
       "      <td>0.450081</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5017 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      venue target authors                                      combined text  \\\n",
       "0  0.043011       [42, 36]  [25, 19, 23, 15, 18, 19, 20, 29, 15, 19, 21, 1...   \n",
       "1  0.004301           [45]  [0, 15, 17, 24, 15, 36, 22, 19, 16, 15, 15, 20...   \n",
       "2  1.000000             []  [0, 15, 17, 24, 15, 26, 15, 15, 20, 20, 17, 22...   \n",
       "3  0.008602           [97]  [0, 16, 15, 1, 17, 16, 0, 0, 34, 15, 27, 15, 1...   \n",
       "4  0.019355            [2]  [0, 37, 38, 16, 22, 17, 30, 19, 15, 18, 17, 0,...   \n",
       "\n",
       "   word_mean  word_count  word_spread  0  1  2  3  ...    1    2    3    4  \\\n",
       "0   0.838297    0.026862     0.472364  0  0  0  0  ...  0.0  0.0  0.0  0.0   \n",
       "1   0.634257    0.035458     0.598874  0  0  0  0  ...  0.0  0.0  0.0  0.0   \n",
       "2   0.623812    0.028653     0.323598  0  0  0  0  ...  0.0  0.0  0.0  0.0   \n",
       "3   0.775345    0.032593     0.693761  0  0  0  0  ...  0.0  0.0  0.0  0.0   \n",
       "4   0.740513    0.043338     0.450081  0  0  0  0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "     5    6    7    8    9  label  \n",
       "0  0.0  1.0  0.0  0.0  0.0      0  \n",
       "1  0.0  0.0  1.0  0.0  0.0      0  \n",
       "2  0.0  0.0  0.0  1.0  1.0      0  \n",
       "3  0.0  0.0  0.0  0.0  0.0      0  \n",
       "4  0.0  0.0  0.0  0.0  1.0      0  \n",
       "\n",
       "[5 rows x 5017 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cea9eb45-a581-49ab-8751-e49f7e920b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original examples of the abstract include:\n",
    "\n",
    "#[25, 19, 23, 15, 18, 19, 20, 29, 15, 19, 21, 15, 26, 42, 0, 26, 15, 18, 16, 29, 36, 19, 17, 20, 37, 15, 16, 15, 0, \n",
    "#44, 15, 15, 20, 20, 15, 0, 17, 15, 15, 16, 15, 16, 47, 0, 17, 42, 26, 16, 17, 16, 25, 15, 33, 16, 44, 22, 15, 16,\n",
    "#38, 25, 15, 19, 17, 25, 46, 15, 16, 38, 25, 15, 34, 0, 0, 17, 16, 42, 16, 15, 33, 16, 44, 18, 47, 19, 21, 24, 0]\n",
    "#label_tensor tensor([1])\n",
    "\n",
    "#[0, 15, 17, 24, 15, 36, 22, 19, 16, 15, 15, 20, 17, 29, 15, 22, 26, 1, 20, 46, 0, 0, 0, 0, 15, 15, 0, 35, 22, 17, \n",
    "#0, 16, 1, 46, 22, 15, 22, 20, 19, 29, 15, 22, 0, 29, 20, 15, 25, 0, 37, 42, 15, 41, 40, 40, 15, 16, 15, 17, 0, 15, \n",
    "#17, 35, 22, 22, 15, 15, 15, 0, 15, 16, 22, 17, 22, 0, 0, 0, 16, 16, 15, 28, 15, 15, 0, 0, 35, 0, 20, 17, 48, 30, 15, \n",
    "#25, 15, 43, 0, 0, 0, 0, 15, 0, 0, 24, 15, 15, 48, 0, 21, 0, 21, 18, 0]\n",
    "#label_tensor tensor([0])\n",
    "\n",
    "#[0, 15, 17, 24, 15, 26, 15, 15, 20, 20, 17, 22, 18, 15, 16, 16, 0, 0, 16, 17, 0, 0, 17, 18, 23, 19, 16, 19, 15,   \n",
    "#15, 25, 24, 15, 19, 15, 15, 38, 0, 22, 17, 15, 19, 20, 21, 15, 21, 17, 26, 0, 0, 15, 17, 16, 22, 17, 15, 18, 23,  \n",
    "#17, 0, 20, 0, 21, 16, 16, 0, 0, 21, 39, 16, 36, 33, 49, 0, 15, 16, 36, 18, 16, 19, 16, 17, 0, 17, 18, 34, 17, 16,\n",
    "#17, 19, 28, 0]\n",
    "#label_tensor tensor([0])\n",
    "\n",
    "#[0, 16, 15, 1, 17, 16, 0, 0, 34, 15, 27, 15, 18, 32, 15, 36, 15, 16, 20, 0, 25, 48, 17, 32, 17, 19, 24, 0, 19, 15,\n",
    "#30, 27, 0, 0, 16, 15, 0, 42, 0, 0, 18, 43, 28, 41, 31, 17, 0, 31, 15, 27, 18, 32, 0, 0, 19, 20, 15, 29, 17, 15, 26, \n",
    "#43, 15, 19, 18, 0, 42, 0, 17, 31, 33, 44, 48, 48, 20, 0, 31, 26, 27, 15, 47, 35, 0, 0, 49, 16, 15, 0, 45, 26, 15, 34, \n",
    "#20, 15, 0, 30, 19, 15, 18, 15, 20, 22, 0]\n",
    "#label_tensor tensor([0])\n",
    "\n",
    "#[0, 37, 38, 16, 22, 17, 30, 19, 15, 18, 17, 0, 16, 17, 30, 16, 41, 15, 26, 26, 16, 33, 34, 18, 1, 34, 30, 0, 18, 0, 18, \n",
    "#16, 0, 15, 32, 15, 49, 15, 24, 17, 16, 34, 39, 0, 0, 46, 17, 19, 18, 17, 18, 16, 28, 15, 19, 20, 22, 17, 41, 0, 0, 25, \n",
    "#16, 21, 18, 15, 21, 22, 17, 30, 15, 19, 17, 27, 15, 17, 28, 0, 0, 44, 15, 21, 17, 21, 15, 19, 17, 18, 41, 19, 0, 38, 17, \n",
    "#18, 17, 24, 19, 15, 19, 15, 15, 36, 24, 18, 0, 0, 17, 16, 27, 21, 18, 25, 15, 0, 16, 0, 41, 15, 17, 32, 28, 0, 16, 16, 16, \n",
    "#15, 27, 27, 15, 17, 17, 28, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f6c96be-64e1-4ae3-86ca-d3785fe72752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95fe96-dd43-40e4-a5c5-4741994b52a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
